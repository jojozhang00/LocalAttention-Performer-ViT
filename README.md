# Transformers as visual encoders

This project benchmarks different Performer-architectures (Performer-ReLU, Performer-approximate-softmax with different numbers of random features) as well as local-attention Transformer-architectures (with different receptive fields) on the classification tasks (for different datasets: MNIST, CIFAR10, ImageNet, Places365).
